{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Evaluate Quality of Generated Images\n",
    "\n",
    "Next, we will train a CNN to classify whether or not an image was generated or not. A low accuracy (near 50%) will indicate that the generated images are indistinguishable from the original images, while a higher accuracy may indicate noticeable differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load original dataset, label as 0 for \"not generated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "original_dataset = datasets.load_dataset(\"huggan/smithsonian_butterflies_subset\", split=\"train\")\n",
    "\n",
    "image_size=64\n",
    "\n",
    "# Same transform, but no horizontal flip to conserve original image\n",
    "eval_preprocess=transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),  # Resize\n",
    "        transforms.ToTensor(),  # Convert to tensor (0, 1)\n",
    "        transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)\n",
    "    ])\n",
    "\n",
    "# Define function for transformation and labeling as original images\n",
    "def transform_and_label(dataset):\n",
    "    # Transform original images\n",
    "    transformed_images = [eval_preprocess(image.convert(\"RGB\")) for image in dataset[\"image\"]]\n",
    "    original_labels = torch.zeros(len(transformed_images), dtype=torch.long)  # Label for original images\n",
    "    return {\"images\": torch.stack(transformed_images), \"labels\": original_labels}\n",
    "\n",
    "original_dataset.set_transform(transform_and_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate images, label as 1 for \"generated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(num_images, model, scheduler, device):\n",
    "    # Start with random initializations for the entire batch\n",
    "    samples = torch.randn(num_images, 3, 64, 64).to(device)\n",
    "\n",
    "    # Iterate through the timesteps defined by the scheduler\n",
    "    for t in scheduler.timesteps:\n",
    "        with torch.no_grad():\n",
    "            # Get model predictions for this timestep\n",
    "            noise_pred = model(samples, t).sample\n",
    "\n",
    "        # Update samples with the noise model's step function\n",
    "        samples = scheduler.step(noise_pred, t, samples).prev_sample\n",
    "\n",
    "    # Normalize the images to be between 0 and 1 for consistency and visualization\n",
    "    generated_images = samples.clip(-1, 1) * 0.5 + 0.5\n",
    "\n",
    "    return generated_images\n",
    "\n",
    "num_generated = 10  # Define how many images you want to generate for testing purposes\n",
    "scheduler = diffusers.DDPMScheduler(num_train_timesteps=1000, beta_start=0.001, beta_end=0.02, beta_schedule=\"linear\")\n",
    "generated_images = generate_images(num_generated, model, scheduler, device)\n",
    "generated_labels = torch.ones(num_generated, dtype=torch.long)  # Label for generated images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split original dataset to match number of generated samples, combine\n",
    "Ideally, we would generate the same number of samples as the size of the training set, but we do not have enough memory to generate 1000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Define the size of the split\n",
    "total_size = len(original_dataset)\n",
    "\n",
    "# Split the dataset randomly, ensure split is same size as number of generated samples\n",
    "split_data, _ = random_split(original_dataset, [num_generated, total_size - num_generated])\n",
    "\n",
    "# Combine datasets\n",
    "combined_images = torch.cat([split_data[\"images\"], generated_images], dim=0)\n",
    "combined_labels = torch.cat([split_data[\"labels\"], generated_labels], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class CombinedDataset(TensorDataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "# Create dataset instance\n",
    "full_dataset = CombinedDataset(combined_images, combined_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split combined dataset into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into 80% training and 20% test\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders for Train and Test datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(16 * 16 * 16, 2)  # Adjust size accordingly\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.act1(self.conv1(x)))\n",
    "        x = x.view(-1, 16 * 16 * 16)  # Flatten\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "cnn_model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # Loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for data in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = cnn_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_dataloader)}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate accuracy of CNN for classifying generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_loader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Test the model\n",
    "test_accuracy = evaluate_accuracy(test_loader, cnn_model)\n",
    "print(f'Accuracy of the model on the test images: {test_accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self, verbose=False):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(32, 128, 3)\n",
    "#         self.conv3 = nn.Conv2d(128, 256, 3)\n",
    "#         self.fc1 = nn.Linear(256, 256)\n",
    "#         self.fc2 = nn.Linear(256, 10)\n",
    "#         self.verbose = verbose\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         if self.verbose:\n",
    "#           print(f\"Input shape: {x.size()}\")\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         if self.verbose:\n",
    "#           print(f\"After Layer 1: {x.size()}\")\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         if self.verbose:\n",
    "#           print(f\"After Layer 2: {x.size()}\")\n",
    "#         x = self.pool(F.relu(self.conv3(x)))\n",
    "#         if self.verbose:\n",
    "#           print(f\"After Layer 3: {x.size()}\")\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         if self.verbose:\n",
    "#           print(f\"Flattened: {x.size()}\")\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         if self.verbose:\n",
    "#           print(f\"After first Fully connected layer: {x.size()}\")\n",
    "#         x = self.fc2(x)\n",
    "#         if self.verbose:\n",
    "#           print(f\"Output shape: {x.size()}\")\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Evaluate Quality of Generated Images\n",
    "\n",
    "Next, we will train a CNN to classify whether or not an image was generated or not. A low accuracy (near 50%) will indicate that the generated images are indistinguishable from the original images, while a higher accuracy may indicate noticeable differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load original dataset, label as 0 for \"not generated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = datasets.load_dataset(\"huggan/smithsonian_butterflies_subset\", split=\"train\")\n",
    "\n",
    "image_size=64\n",
    "\n",
    "# Same transform, but no horizontal flip to conserve original image\n",
    "eval_preprocess=transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),  # Resize\n",
    "        transforms.ToTensor(),  # Convert to tensor (0, 1)\n",
    "        transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)\n",
    "    ])\n",
    "\n",
    "# Define function for transformation and labeling as original images\n",
    "def transform_and_label(dataset):\n",
    "    # Transform original images\n",
    "    transformed_images = [eval_preprocess(image.convert(\"RGB\")) for image in dataset[\"image\"]]\n",
    "    original_labels = torch.zeros(len(transformed_images), dtype=torch.long)  # Label for original images\n",
    "    return {\"images\": torch.stack(transformed_images), \"labels\": original_labels}\n",
    "\n",
    "original_dataset.set_transform(transform_and_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate images, label as 1 for \"generated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(num_images, model, scheduler, device):\n",
    "    # Start with random initializations for the entire batch\n",
    "    samples = torch.randn(num_images, 3, 64, 64).to(device)\n",
    "\n",
    "    # Iterate through the timesteps defined by the scheduler\n",
    "    for t in scheduler.timesteps:\n",
    "        with torch.no_grad():\n",
    "            # Get model predictions for this timestep\n",
    "            noise_pred = model(samples, t).sample\n",
    "\n",
    "        # Update samples with the noise model's step function\n",
    "        samples = scheduler.step(noise_pred, t, samples).prev_sample\n",
    "\n",
    "    # Normalize the images to be between 0 and 1 for consistency and visualization\n",
    "    generated_images = samples.clip(-1, 1) * 0.5 + 0.5\n",
    "\n",
    "    return generated_images\n",
    "\n",
    "num_generated = 10  # Define how many images you want to generate for testing purposes\n",
    "generated_images = generate_images(num_generated, model, scheduler, device)\n",
    "generated_labels = torch.ones(num_generated, dtype=torch.long)  # Label for generated images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split original dataset to match number of generated samples, combine\n",
    "Ideally, we would generate the same number of samples as the size of the training set, but we do not have enough memory to generate 1000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the split\n",
    "total_size = len(original_dataset)\n",
    "\n",
    "# Split the dataset randomly, ensure split is same size as number of generated samples\n",
    "split_data, _ = random_split(original_dataset, [num_generated, total_size - num_generated])\n",
    "\n",
    "# Combine datasets\n",
    "combined_images = torch.cat([split_data[\"images\"], generated_images], dim=0)\n",
    "combined_labels = torch.cat([split_data[\"labels\"], generated_labels], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class CombinedDataset(TensorDataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "# Create dataset instance\n",
    "full_dataset = CombinedDataset(combined_images, combined_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split combined dataset into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Split the dataset into 80% training and 20% test\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders for Train and Test datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(16 * 16 * 16, 2)  # Adjust size accordingly\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.act1(self.conv1(x)))\n",
    "        x = x.view(-1, 16 * 16 * 16)  # Flatten\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "cnn_model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # Loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for data in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = cnn_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_dataloader)}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate accuracy of CNN for classifying generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_loader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Test the model\n",
    "test_accuracy = evaluate_accuracy(test_loader, cnn_model)\n",
    "print(f'Accuracy of the model on the test images: {test_accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self, verbose=False):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(32, 128, 3)\n",
    "#         self.conv3 = nn.Conv2d(128, 256, 3)\n",
    "#         self.fc1 = nn.Linear(256, 256)\n",
    "#         self.fc2 = nn.Linear(256, 10)\n",
    "#         self.verbose = verbose\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         if self.verbose:\n",
    "#           print(f\"Input shape: {x.size()}\")\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         if self.verbose:\n",
    "#           print(f\"After Layer 1: {x.size()}\")\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         if self.verbose:\n",
    "#           print(f\"After Layer 2: {x.size()}\")\n",
    "#         x = self.pool(F.relu(self.conv3(x)))\n",
    "#         if self.verbose:\n",
    "#           print(f\"After Layer 3: {x.size()}\")\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         if self.verbose:\n",
    "#           print(f\"Flattened: {x.size()}\")\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         if self.verbose:\n",
    "#           print(f\"After first Fully connected layer: {x.size()}\")\n",
    "#         x = self.fc2(x)\n",
    "#         if self.verbose:\n",
    "#           print(f\"Output shape: {x.size()}\")\n",
    "#         return x\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
